{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6224090c",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b6348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import scipy.stats.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "pd.options.mode.chained_assignment = None # default='warn' #this is for hide warn\n",
    "\n",
    "from optbinning import OptimalBinning # Reference: http://gnpalencia.org/optbinning/tutorials/tutorial_binary.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb7c7d",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e831c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accepts dataset\n",
    "path_accepts = r'\\\\neptune\\RAD\\4 Models\\Scorecard 7.0\\Modeling Data\\JM\\Valid FICO CA Hit\\Accepts_4935.xlsx'\n",
    "df_accepts = pd.read_excel(path_accepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35bf2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rejected dataset\n",
    "path_rejects = r\"\\\\neptune\\RAD\\4 Models\\Scorecard 7.0\\Modeling Data\\JM\\Valid FICO CA Hit\\Rejects_28269.csv\"\n",
    "df_rejects = pd.read_csv(path_rejects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519095f9",
   "metadata": {},
   "source": [
    "# Adding Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27aa2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Frame\n",
    "df = df_accepts\n",
    "\n",
    "# Target\n",
    "target = \"G_B\"\n",
    "key = 'LNKEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9542df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Frequency\n",
    "Count_Bad = df.groupby(target).count().loc['BAD',key]\n",
    "Count_Good_INDET = df.groupby(target).count().loc[['GOOD','INDET'],key].sum()\n",
    "if Count_Good_INDET > Count_Bad:\n",
    "    freq_dic = {'GOOD':Count_Good_INDET/Count_Bad,'BAD':1}\n",
    "    frequency = freq_dic['GOOD']\n",
    "else:\n",
    "    freq_dic = {'GOOD':1,'BAD':Count_Bad/Count_Good_INDET}\n",
    "    frequency = freq_dic['BAD']\n",
    "\n",
    "# Uncomment to see values\n",
    "# freq_dic\n",
    "# frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d45a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Frequency to Data Frame\n",
    "def Add_Frequency(row,freq_dic):\n",
    "    if row[target] == 'BAD':\n",
    "        return freq_dic['BAD']\n",
    "    else:\n",
    "        return freq_dic['GOOD']\n",
    "\n",
    "df['Frequency'] = df.apply(lambda row : Add_Frequency(row,freq_dic), axis=1)\n",
    "\n",
    "# Uncomment to see values\n",
    "#df[[target,'Frequency']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936207b5",
   "metadata": {},
   "source": [
    "# Filtering, Sampling, and Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2a2f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good: 2286 Bad: 2207\n"
     ]
    }
   ],
   "source": [
    "# Filtering by Good and Bad\n",
    "Goods=df[df[target]==\"GOOD\"] \n",
    "Bads=df[df[target]==\"BAD\"]\n",
    "\n",
    "# Uncomment to see values\n",
    "print(\"Good:\",len(Goods), \"Bad:\",len(Bads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16bb895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good: 2207 Bad: 2207\n"
     ]
    }
   ],
   "source": [
    "# Sampling Down\n",
    "if len(Goods) >= len(Bads):\n",
    "    Bad = Bads\n",
    "    Good = Goods.sample(len(Bad),random_state =2602)\n",
    "    print(\"Good:\",len(Good),\"Bad:\",len(Bad))\n",
    "else:\n",
    "    Good = Goods\n",
    "    Bad = Bads.sample(len(Good),random_state=2602)\n",
    "    print(\"Good:\",len(Good),\"Bad:\",len(Bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb86823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended Good and Bad count: 4414\n"
     ]
    }
   ],
   "source": [
    "# Appending\n",
    "\n",
    "appended = Good.append(Bad) \n",
    "\n",
    "print(\"Appended Good and Bad count:\",len(appended)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb72eae",
   "metadata": {},
   "source": [
    "# Removing Unwanted Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753974d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Objects\n",
    "Objects = appended.dtypes[appended.dtypes=='object'].index.tolist()\n",
    "#Objects # Copy and paste these columns into more_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "910f2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all categorical variable and unnecessary columns\n",
    "remove_columns = appended.dtypes[appended.dtypes=='datetime64[ns]'].index.tolist() # Removing date/time variables\n",
    "more_columns =[\n",
    "'LNKEY',\n",
    " 'BY1_SEG',\n",
    " 'BY1_GRADE',\n",
    " 'BRANCH',\n",
    " 'STA',\n",
    " 'DEL_STA',\n",
    " 'P',\n",
    " 'MAKE',\n",
    " 'MODEL',\n",
    " 'INCOME_TYPE',\n",
    " 'EMPLOYER_NAME',\n",
    " 'JOB_POSITION',\n",
    " 'LGL_ST',\n",
    " 'CLOSE_CODE',\n",
    " 'Decision_Status',\n",
    " 'PMTs_Default',\n",
    " 'Model_State',\n",
    " 'LNKey',\n",
    " 'SC60_Segment',\n",
    " 'lnkey',\n",
    " 'LL_BAD_ABA',\n",
    " 'LL_BNKACT_MISMATCH',\n",
    " 'LL_BANKRUPTCY',\n",
    " 'LL_DL_MISMATCH',\n",
    " 'LL_PAY_FREQ_MISMATCH',\n",
    " 'LL_RESIDENCE_STATE_MISMATCH',\n",
    " 'LL_SSN_MISMATCH',\n",
    " 'ACH_BI1_BankAccountID',\n",
    " 'ACH_BI1_BUC1_Description',\n",
    " 'ACH_BI1_BUC2_Description',\n",
    " 'ACH_BI1_BUC3_Description',\n",
    " 'ACH_BI1_BUC4_Description',\n",
    " 'ACH_BI1_BUC5_Description',\n",
    " 'inputprovidedssn',\n",
    " 'ACH_BI1_BUC6_Description',\n",
    " 'ACH_BI1_BUC7_Description',\n",
    " 'Client Data',\n",
    " 'Client Data.1',\n",
    " 'LL_last_dt_APPL_RCVD',\n",
    " 'LL_OL_last_dt_APPL_RCVD',\n",
    " 'LL_SF_last_dt_APPL_RCVD',\n",
    " 'LNKEY',\n",
    " 'LNKey',\n",
    " 'lnkey',\n",
    " 'Contract_Open_Date',\n",
    " 'BY1_SCORE',\n",
    " 'SCOREX',\n",
    " 'BY1_SEG',\n",
    " 'BY1_GRADE',\n",
    " 'BY2_SCORE',\n",
    " 'CASH_DOWNPCT',\n",
    " 'NET_AFPCT',\n",
    " 'YRSATWORK',\n",
    " 'BYRAGE',\n",
    " 'ANNUAL_RATE',\n",
    " 'NAMTF_LFC',\n",
    " 'BRANCH',\n",
    " 'LOT_ID',\n",
    " 'CHECK_DATE',\n",
    " 'STA',\n",
    " 'DEL_STA',\n",
    " 'X_BOOK',\n",
    " 'P',\n",
    " 'RECOURSE',\n",
    " 'DISC_PC',\n",
    " 'INVOICE_BOOK_VALUE',\n",
    " 'inputprovidedphone',\n",
    " 'NADA_VALUE',\n",
    " 'VHCL_YEAR',\n",
    " 'MAKE',\n",
    " 'MODEL',\n",
    " 'ODO',\n",
    " 'INCOME_TYPE',\n",
    " 'GROSS_MONTH',\n",
    " 'EMPLOYER_NAME',\n",
    " 'JOB_POSITION',\n",
    " 'LGL_ST',\n",
    " 'DOWN',\n",
    " 'PAYMENT',\n",
    " 'Buyer_1_MOSATWORK',\n",
    " 'Buyer_2_MOSATWORK',\n",
    " 'CTRL_ZIP',\n",
    " 'Advance',\n",
    " 'BYR1_FICO',\n",
    " 'AMTF_LFC',\n",
    " 'DECRULESET',\n",
    " 'CAR_AGE',\n",
    " 'OPENINGBALANCE_PCT',\n",
    " 'AMTF_LFC_PCT_Risk',\n",
    " 'AUTOMOBILE_PRICE',\n",
    " 'ACV_VALUE',\n",
    " 'Valid_SSN_Indicator',\n",
    " 'CLOSE_DATE',\n",
    " 'CLOSE_CODE',\n",
    " 'GAIN',\n",
    " 'DR_ORIGBAL',\n",
    " 'DAYS_LATE',\n",
    " 'PAIDMO',\n",
    " 'TERM',\n",
    " 'FIRST_PMT_PAYMENT',\n",
    " 'Decision_Status',\n",
    " 'PMTs_Default',\n",
    " 'PTI',\n",
    " 'LTVPCT_XBOOK',\n",
    " 'APR_ANNUAL_RATE',\n",
    " 'AMOUNT_FINANCED',\n",
    " 'BAD_CLOSECODE_Indicator',\n",
    " 'OPEN_YEARMONTH',\n",
    " 'CLOSE_YEARMONTH',\n",
    " 'SC60_Segment',\n",
    " 'SC60_Stage1',\n",
    " 'SC60_Stage2',\n",
    " 'SQL_CASHDOWNPCT',\n",
    " 'SQL_OPENINGBALANCE_PCT',\n",
    " 'SQL_PTI',\n",
    " 'SQL_LTV',\n",
    " 'ft_appdt_match',\n",
    " 'ft_match',\n",
    " 'Frequency',\n",
    " 'ssndatelowissued',\n",
    " 'subjectage',\n",
    " 'BAD_CLOSECODE_Indicator']\n",
    "\n",
    "remove_columns.extend(more_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9be49e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auto_score</th>\n",
       "      <th>bankcard_score</th>\n",
       "      <th>short_term_lending_score</th>\n",
       "      <th>telecommunications_score</th>\n",
       "      <th>crossindustry_score</th>\n",
       "      <th>subjectrecordtimeoldest</th>\n",
       "      <th>subjectrecordtimenewest</th>\n",
       "      <th>subjectnewestrecord12month</th>\n",
       "      <th>subjectactivityindex03month</th>\n",
       "      <th>subjectactivityindex06month</th>\n",
       "      <th>...</th>\n",
       "      <th>UTI0300</th>\n",
       "      <th>UTI0436</th>\n",
       "      <th>UTI2388</th>\n",
       "      <th>UTI4180</th>\n",
       "      <th>UTI5030</th>\n",
       "      <th>UTI6200</th>\n",
       "      <th>UTI6280</th>\n",
       "      <th>UTI8151</th>\n",
       "      <th>UTI8320</th>\n",
       "      <th>G_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>648.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3399</th>\n",
       "      <td>699.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>674.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>650.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>767.0</td>\n",
       "      <td>777.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4928</th>\n",
       "      <td>664.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>BAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>667.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>615.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "      <td>BAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4932</th>\n",
       "      <td>694.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>668.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>BAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>732.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4414 rows Ã— 2232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auto_score  bankcard_score  short_term_lending_score  \\\n",
       "458        648.0           648.0                     563.0   \n",
       "3399       699.0           770.0                     587.0   \n",
       "3876       674.0           750.0                     582.0   \n",
       "3671       650.0           725.0                     568.0   \n",
       "2021       767.0           777.0                     651.0   \n",
       "...          ...             ...                       ...   \n",
       "4928       664.0           757.0                     566.0   \n",
       "4929       667.0           726.0                     573.0   \n",
       "4930       615.0           671.0                     501.0   \n",
       "4932       694.0           752.0                     575.0   \n",
       "4933       732.0           738.0                     627.0   \n",
       "\n",
       "      telecommunications_score  crossindustry_score  subjectrecordtimeoldest  \\\n",
       "458                      554.0                599.0                    134.0   \n",
       "3399                     605.0                589.0                    296.0   \n",
       "3876                     574.0                647.0                    112.0   \n",
       "3671                     591.0                553.0                     32.0   \n",
       "2021                     757.0                701.0                    397.0   \n",
       "...                        ...                  ...                      ...   \n",
       "4928                     547.0                588.0                    143.0   \n",
       "4929                     580.0                664.0                    133.0   \n",
       "4930                     538.0                563.0                    404.0   \n",
       "4932                     639.0                668.0                    258.0   \n",
       "4933                     694.0                699.0                    341.0   \n",
       "\n",
       "      subjectrecordtimenewest  subjectnewestrecord12month  \\\n",
       "458                       1.0                         1.0   \n",
       "3399                      1.0                         1.0   \n",
       "3876                      1.0                         1.0   \n",
       "3671                      1.0                         1.0   \n",
       "2021                      1.0                         1.0   \n",
       "...                       ...                         ...   \n",
       "4928                      1.0                         1.0   \n",
       "4929                      1.0                         1.0   \n",
       "4930                      1.0                         1.0   \n",
       "4932                      1.0                         1.0   \n",
       "4933                      1.0                         1.0   \n",
       "\n",
       "      subjectactivityindex03month  subjectactivityindex06month  ...  UTI0300  \\\n",
       "458                           4.0                          4.0  ...      0.0   \n",
       "3399                          4.0                          4.0  ...      0.0   \n",
       "3876                          4.0                          4.0  ...      0.0   \n",
       "3671                          4.0                          4.0  ...      0.0   \n",
       "2021                          3.0                          3.0  ...      0.0   \n",
       "...                           ...                          ...  ...      ...   \n",
       "4928                          3.0                          4.0  ...      1.0   \n",
       "4929                          4.0                          4.0  ...      0.0   \n",
       "4930                          4.0                          4.0  ...      2.0   \n",
       "4932                          4.0                          4.0  ...      1.0   \n",
       "4933                          4.0                          4.0  ...      0.0   \n",
       "\n",
       "      UTI0436  UTI2388  UTI4180  UTI5030  UTI6200  UTI6280  UTI8151  UTI8320  \\\n",
       "458       NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3399      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3876      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3671      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2021      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "4928      0.0      1.0      0.0      NaN    400.0    400.0      NaN     21.0   \n",
       "4929      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4930      0.0      0.0      0.0      NaN    400.0      0.0      NaN     38.0   \n",
       "4932      0.0      0.0      0.0      NaN    400.0      1.0      NaN     63.0   \n",
       "4933      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "       G_B  \n",
       "458   GOOD  \n",
       "3399  GOOD  \n",
       "3876  GOOD  \n",
       "3671  GOOD  \n",
       "2021  GOOD  \n",
       "...    ...  \n",
       "4928   BAD  \n",
       "4929   BAD  \n",
       "4930   BAD  \n",
       "4932   BAD  \n",
       "4933   BAD  \n",
       "\n",
       "[4414 rows x 2232 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe for grouping\n",
    "df_for_groups = appended.drop(remove_columns,axis=1)\n",
    "df_for_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8969d41",
   "metadata": {},
   "source": [
    "# Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "420a4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Frequency list to be used later\n",
    "frequency = appended['Frequency']\n",
    "#frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d375b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2dda586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train Count: 3089\n",
      "X Test Count: 1325\n",
      "X Total: 4414\n",
      "Y Train Count: 3089\n",
      "Y Test Count: 1325\n",
      "Y Total: 4414\n"
     ]
    }
   ],
   "source": [
    "y = pd.get_dummies(appended[target])\n",
    "y_Good = y[\"GOOD\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_for_groups, y_Good, random_state = random_seed, test_size=0.3) # test_size sets up ratio\n",
    "\n",
    "print('X Train Count:',len(X_train))\n",
    "print('X Test Count:',len(X_test))\n",
    "print('X Total:',len(X_train)+len(X_test))\n",
    "\n",
    "print('Y Train Count:',len(y_train))\n",
    "print('Y Test Count:',len(y_test))\n",
    "print('Y Total:',len(y_train)+len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c592e",
   "metadata": {},
   "source": [
    "# Interactive Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bfe9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Target\n",
    "X_train = X_train.drop([target],axis=1)\n",
    "n = 5\n",
    "reject_level = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74023375",
   "metadata": {},
   "source": [
    "### Interactive Grouping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ceaae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "X_train['useless_column'] = 1\n",
    "columns_name = X_train.columns\n",
    "y_name = y_train.name\n",
    "freq_name = frequency.name\n",
    "group_dictionary={} #creates an empty dictionary\n",
    "IV_dictionary = {} #creates an empty dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec98565",
   "metadata": {},
   "source": [
    "### For Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d9535a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each columns \n",
    "i = 1\n",
    "# Not Missing\n",
    "temp_not_missing = X_train.loc[X_train[columns_name[i]].notnull(), [columns_name[i],'useless_column']] \n",
    "not_missing_with_y = pd.concat([temp_not_missing,y_train],join='inner',axis=1)\n",
    "not_missing_with_y_fre = pd.concat([not_missing_with_y,frequency],join='inner',axis=1)\n",
    "not_missing_with_y_fre[\"freq*Good\"] = not_missing_with_y_fre[freq_name]*not_missing_with_y_fre[y_name]\n",
    "\n",
    "# Missing\n",
    "temp_missing = X_train.loc[X_train[columns_name[i]].isnull(), columns_name[i]]\n",
    "missing_with_y = pd.concat([temp_missing,y_train],join='inner',axis=1)\n",
    "missing_with_y_fre = pd.concat([missing_with_y,frequency],join='inner',axis=1)\n",
    "missing_with_y_fre[\"freq*Good\"] = missing_with_y_fre[freq_name]*missing_with_y_fre[y_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a466a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting loop variables\n",
    "r = 0 #this is to reset the while loop when starting a variable i.e it will do FICO then in order to do cc2_ad we want everything to reset\n",
    "p = 0 #this is to reset the while loop when starting a variable i.e it will do FICO then in order to do cc2_ad we want everything to reset\n",
    "u = 0 #this is to reset the while loop when starting a variable i.e it will do FICO then in order to do cc2_ad we want everything to reset\n",
    "m = n # this is to reset m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87eefdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "1193    1\n",
       "1030    1\n",
       "3853    1\n",
       "2252    1\n",
       "1735    1\n",
       "       ..\n",
       "2846    5\n",
       "2377    5\n",
       "380     5\n",
       "1385    5\n",
       "911     5\n",
       "Name: Bucket_#, Length: 3053, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_not_missing = not_missing_with_y_fre[columns_name[i]]\n",
    "t_name = temp_not_missing.name #column name\n",
    "a = temp_not_missing.copy() #copy the dataframe. Note this \"a\" is not the same \"a\" from above\n",
    "a = pd.concat([a,frequency],join='inner',axis=1) #combine the column with the frequency column\n",
    "a = a.sort_values(by = t_name) #sorts the column name from smallest to largest\n",
    "a['cumulative sum'] = a[frequency.name].cumsum() #calculates the cumulative sum of the frequency\n",
    "interval = (a['cumulative sum'].max()+0.05)/m #find the max of the cumulative sum and adds 0.5 to it and then divides it by m which is the number of buckets\n",
    "b = {'Bucket_#':[],'Bucket_max':[]} #emtpy lists\n",
    "\n",
    "for i in range(1,m+1): #the list will only start at 1 and end at m. It will not include m+1\n",
    "    b['Bucket_#'].append(i) #b will keep getting appended with just the values of i. \n",
    "    b['Bucket_max'].append(a.loc[a['cumulative sum'] <= i*interval, t_name].max()) #this will check the max for each interval. Example, interval 1 for FICO can only go up to FICO score 418\n",
    "\n",
    "c = pd.DataFrame(b) #creates the data fram with bucket number and the bucket max for each bucket number\n",
    "c['Bucket_max'] = c['Bucket_max'].fillna(a[t_name].min()) #if the row is blank for a column then just include it in the minimum bin\n",
    "temp_not_missing = pd.concat([temp_not_missing,frequency],join='inner',axis=1) #combines column name with frequency\n",
    "temp_not_missing = temp_not_missing.reset_index() #reset the index of the dataframe \n",
    "temp_not_missing = temp_not_missing.sort_values(by = t_name) #sorts it by small to large by column name\n",
    "temp_not_missing[t_name] = temp_not_missing[t_name].astype('float64') #they all have the same data type\n",
    "c['Bucket_max'] = c['Bucket_max'].astype('float64') #they all have the same data type\n",
    "if len(temp_not_missing) == 0: #if temp_not_missing includes nothing print variable name includes Nan only\n",
    "    print(t_name + ' are all Nan')\n",
    "else: \n",
    "    d = pd.merge_asof(temp_not_missing, \\\n",
    "    c.sort_values('Bucket_max'), \\\n",
    "    left_on = t_name, right_on = 'Bucket_max',direction = 'forward').set_index('index')\n",
    "d['Bucket_#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d537f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create quartile_cut function\n",
    "def quartile_cut(temp_not_missing, frequency, m):\n",
    "    t_name = temp_not_missing.name #column name\n",
    "    a = temp_not_missing.copy() #copy the dataframe. Note this \"a\" is not the same \"a\" from above\n",
    "    a = pd.concat([a,frequency],join='inner',axis=1) #combine the column with the frequency column\n",
    "    a = a.sort_values(by = t_name) #sorts the column name from smallest to largest\n",
    "    a['cumulative sum'] = a[frequency.name].cumsum() #calculates the cumulative sum of the frequency\n",
    "    interval = (a['cumulative sum'].max()+0.05)/m #find the max of the cumulative sum and adds 0.5 to it and then divides it by m which is the number of buckets\n",
    "    b = {'Bucket_#':[],'Bucket_max':[]} #emtpy lists\n",
    "    for i in range(1,m+1): #the list will only start at 1 and end at m. It will not include m+1\n",
    "        b['Bucket_#'].append(i) #b will keep getting appended with just the values of i. \n",
    "        b['Bucket_max'].append(a.loc[a['cumulative sum'] <= i*interval, t_name].max()) #this will check the max for each interval. Example, interval 1 for FICO can only go up to FICO score 418\n",
    "    c = pd.DataFrame(b) #creates the data fram with bucket number and the bucket max for each bucket number\n",
    "    c['Bucket_max'] = c['Bucket_max'].fillna(a[t_name].min()) #if the row is blank for a column then just include it in the minimum bin\n",
    "    temp_not_missing = pd.concat([temp_not_missing,frequency],join='inner',axis=1) #combines column name with frequency\n",
    "    temp_not_missing = temp_not_missing.reset_index() #reset the index of the dataframe \n",
    "    temp_not_missing = temp_not_missing.sort_values(by = t_name) #sorts it by small to large by column name\n",
    "    temp_not_missing[t_name] = temp_not_missing[t_name].astype('float64') #they all have the same data type\n",
    "    c['Bucket_max'] = c['Bucket_max'].astype('float64') #they all have the same data type\n",
    "    if len(temp_not_missing) == 0: #if temp_not_missing includes nothing print variable name includes Nan only\n",
    "        print(t_name + ' are all Nan')\n",
    "    else: \n",
    "        d = pd.merge_asof(temp_not_missing, \\\n",
    "        c.sort_values('Bucket_max'), \\\n",
    "        left_on = t_name, right_on = 'Bucket_max',direction = 'forward').set_index('index')\n",
    "    return d['Bucket_#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "184a610a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "1193     1\n",
       "1030     1\n",
       "3853     1\n",
       "2252     1\n",
       "1735     1\n",
       "        ..\n",
       "2846    20\n",
       "2377    20\n",
       "380     20\n",
       "1385    20\n",
       "911     20\n",
       "Name: Bucket_#, Length: 3053, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartile_cut(not_missing_with_y_fre[columns_name[1]],frequency, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(not_missing_with_y_fre) > 0: #if not_missing_with_y_fre has record\n",
    "    while (np.abs(r) < 0.9 or u <= 100) and m >= 1: #stop until (spearman correlation great than 0.9 and smallest bin has records more than 150) or m = 0\n",
    "        not_missing_with_y_fre[\"Bucket1\"] = quartile_cut(not_missing_with_y_fre[columns_name[i]],frequency, 20) #quartile function from above. This will create a new variable called Bucket which is the bucket interval from quartile cut\n",
    "        if m > 1:\n",
    "            clf = tree.DecisionTreeClassifier(random_state = 0, max_leaf_nodes = m, min_weight_fraction_leaf = 0.045) # *set decision tree parameters (max_leaf_nodes: max number of bins,min_weight_fraction_leaf: min sample % for each bins)*\n",
    "            clf = clf.fit(not_missing_with_y_fre[[\"Bucket1\",'useless_column']], not_missing_with_y_fre[y_name], not_missing_with_y_fre[freq_name].values) # *fit not_missing_with_y_fre*\n",
    "            not_missing_with_y_fre[\"Bucket\"] = clf.apply(not_missing_with_y_fre[[\"Bucket1\",'useless_column']]) # *find bin number for each records*\n",
    "            d2 = not_missing_with_y_fre.groupby('Bucket', as_index = True) #group by function. group by bucket from above\n",
    "            r, p = stats.spearmanr(d2[columns_name[i]].mean(), d2[\"freq*Good\"].sum()/d2[freq_name].sum()) #find the average for the column name in the training data, sum of the good event rate and divides it by the sum of the frequency then calculate spearman correlation\n",
    "            u = d2[y_name].count().min() #returns the count and minimum of the column that gives the output good.\n",
    "            m = m - 1\n",
    "        else: #if not_missing_with_y_fre doesn't have record\n",
    "            not_missing_with_y_fre[\"Bucket\"] = 1\n",
    "            d2 = not_missing_with_y_fre.groupby('Bucket', as_index = True)\n",
    "            m = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_missing_with_y_fre[\"Bucket1\"] = quartile_cut(not_missing_with_y_fre[columns_name[i]],frequency, 20)\n",
    "not_missing_with_y_fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c326957",
   "metadata": {},
   "outputs": [],
   "source": [
    "if m > 1:\n",
    "    clf = tree.DecisionTreeClassifier(random_state = 0, max_leaf_nodes = m, min_weight_fraction_leaf = 0.045) # *set decision tree parameters (max_leaf_nodes: max number of bins,min_weight_fraction_leaf: min sample % for each bins)*\n",
    "    clf = clf.fit(not_missing_with_y_fre[[\"Bucket1\",'useless_column']], not_missing_with_y_fre[y_name], not_missing_with_y_fre[freq_name].values) # *fit not_missing_with_y_fre*\n",
    "    not_missing_with_y_fre[\"Bucket\"] = clf.apply(not_missing_with_y_fre[[\"Bucket1\",'useless_column']]) # *find bin number for each records*\n",
    "    \n",
    "    #Reference: # https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "    \n",
    "    d2 = not_missing_with_y_fre.groupby('Bucket', as_index = True) #group by function. group by bucket from above\n",
    "    r, p = stats.spearmanr(d2[columns_name[i]].mean(), d2[\"freq*Good\"].sum()/d2[freq_name].sum()) #find the average for the column name in the training data, sum of the good event rate and divides it by the sum of the frequency then calculate spearman correlation\n",
    "    \n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "    \n",
    "    u = d2[y_name].count().min() #returns the count and minimum of the column that gives the output good.\n",
    "    m = m - 1\n",
    "else: #if not_missing_with_y_fre doesn't have record\n",
    "    not_missing_with_y_fre[\"Bucket\"] = 1\n",
    "    d2 = not_missing_with_y_fre.groupby('Bucket', as_index = True)\n",
    "    m = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
